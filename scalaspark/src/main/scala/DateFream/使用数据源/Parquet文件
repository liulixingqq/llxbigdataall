2、Parquet文件：是Spark SQL的Load函数默认的数据源
		（*）特点：列式存储文件
		（*）把其他文件格式转成Parquet文件   json文件  ----> parquet文件
		      val empJSON = spark.read.json("/root/temp/emp.json")
		      empJSON.write.mode("overwrite").parquet("/root/temp/result")

		（*）功能：支持Schema的合并
				第一个文件
				 val df1 = sc.makeRDD(1 to 5).map(i=>(i,i*2)).toDF("single","double")
				 df1.write.parquet("/root/temp/test_table/key=1")

				第二个文件
				 val df2 = sc.makeRDD(6 to 10).map(i=>(i,i*3)).toDF("single","triple")
	             df2.write.parquet("/root/temp/test_table/key=2")

				合并上面的文件
				val df3 = spark.read.option("mergeSchema","true").parquet("/root/temp/test_table")

					scala> df3.printSchema
					root
					 |-- single: integer (nullable = true)
					 |-- double: integer (nullable = true)
					 |-- triple: integer (nullable = true)
					 |-- key: integer (nullable = true)


					scala> df3.show
					+------+------+------+---+
					|single|double|triple|key|
					+------+------+------+---+
					|     8|  null|    24|  2|
					|     9|  null|    27|  2|
					|    10|  null|    30|  2|
					|     3|     6|  null|  1|
					|     4|     8|  null|  1|
					|     5|    10|  null|  1|
					|     6|  null|    18|  2|
					|     7|  null|    21|  2|
					|     1|     2|  null|  1|
					|     2|     4|  null|  1|
					+------+------+------+---+